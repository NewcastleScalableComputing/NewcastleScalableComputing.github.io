<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title></title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2017-11-15T20:47:32+00:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name></name>
 </author>
 
 
 <entry>
   <title>Preserving the currency of genomics outcomes over time through selective re-computation&#58; techniques, initial findings, and open challenges</title>
   <link href="http://localhost:4000/upcoming/2017/11/20/missier.html"/>
   <updated>2017-11-20T00:00:00+00:00</updated>
   <id>http://localhost:4000/upcoming/2017/11/20/missier</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Time and Location:&lt;/strong&gt; Monday 20th November 2017 10:45-12:00, Urban Sciences Building USB2.022&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
Complex and computationally expensive processes are common in many areas of bioinformatics, e.g. in genomics and metagenomics. The outcomes of such processes are time-sensitive, as they depend on algorithms, tools, and reference databases which all evolve over time, often independently of one another.  This suggests that some of the processes may need to be repeatedly re-computed in response  to these changes, in order to refresh their outcomes. However, these computations can be expensive, consuming tens of CPU hours each, and not all past cases will be affected by all changes.&lt;/p&gt;

&lt;p&gt;In the ReComp project, we investigate general methods for optimising re-computation of workflow-based processes in response to changes in the underlying reference data. We have chosen a metadata analytics  approach: for each execution, e.g. of a variant calling pipeline, we record metadata (provenance) about the process execution, its inputs, outputs, and dependencies.  These records form an ever-growing history database that we can then use to assess the future impact of observed changes and to optimise the re-computation.&lt;/p&gt;

&lt;p&gt;In this talk we will present a generic selective re-computation framework and how it applies to our genomics case study.&lt;/p&gt;

&lt;p&gt;We outline a number of open research challenges that emerge from our initial investigation, including learning useful predictive models of change impact from the history database.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Visualization Tools and Techniques</title>
   <link href="http://localhost:4000/past/2017/07/17/holliman.html"/>
   <updated>2017-07-17T00:00:00+01:00</updated>
   <id>http://localhost:4000/past/2017/07/17/holliman</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Time and Location:&lt;/strong&gt; Monday 17th July 2017 10:45-12:00, Foyer Space, Ground Floor, The Core&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
Do you want to understand your data? Do you want to create visualizations that explain your data to others? We will briefly review the purpose, use and outputs of visualization (BI style) software tools such as Power BI and Tableau.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>To See What Isn’t There – Visualization of Missing Data</title>
   <link href="http://localhost:4000/past/2017/06/19/fernstad.html"/>
   <updated>2017-06-19T00:00:00+01:00</updated>
   <id>http://localhost:4000/past/2017/06/19/fernstad</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Time and Location:&lt;/strong&gt; Monday 19th June 2017 10:45-12:00, Foyer Space, Ground Floor, The Core&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
Missing data are records that are absent from a data set. They are data that were intended to be recorded, but for some reason were not. Missing data occur in almost any domain and is a common data analysis challenge that causes problems such as biased results and reduced statistical rigour. Although data visualization has great potential to provide invaluable support for the investigation of missing data, missing data challenges are rarely addressed by the visualization society. This talk will cover various concepts and aspects in missing data analysis, suggesting patterns of relevance for gaining further understanding of ‘missingness’ in datasets and present the result of an evaluation of different visual representations of missing data. It will also suggest some directions for designing visualization to support the understanding of ‘missingness’ in data.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sentiment Analysis using Probabilistic Topic Modelling and Unsupervised Deep Learning</title>
   <link href="http://localhost:4000/past/2017/06/05/mcgough.html"/>
   <updated>2017-06-05T00:00:00+01:00</updated>
   <id>http://localhost:4000/past/2017/06/05/mcgough</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Time and Location:&lt;/strong&gt; Monday 5th June 2017 10:45-12:00, Foyer Space, Ground Floor, The Core&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
It is estimated that 85% of worldwide data is held in unstructured/unlabelled formats - increasing at a rate of roughly 7 million digital pages per day. Exploiting these large datasets can open the door for providing policy makers, corporations, and end-users with unprecedented knowledge for better planning, decision making, and new services. Deep learning and probabilistic topic modelling have shown great potential for analysing such datasets. This analysis helps in: discovering anomalies within these datasets, unravelling underlying patterns/trends, or finding similar texts within a dataset. We’ll illustrate how we can use a combined unsupervised deep learning and topic modelling approach for sentiment analysis requiring minimal feature engineering or prior assumptions, and outperforming the state of the art approaches to sentiment analysis.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Automating Computational Placement in IoT Environments</title>
   <link href="http://localhost:4000/past/2017/05/08/michalak.html"/>
   <updated>2017-05-08T00:00:00+01:00</updated>
   <id>http://localhost:4000/past/2017/05/08/michalak</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Time and Location:&lt;/strong&gt; Monday 8th May 2017 10:45-12:00, Location TBC&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
This work in progress talk will provide an overview of a project designed to deliver an optimised execution plan for IoT environments based on a high level declarative description of computation, current state of infrastructure and a set of non-functional requirements. The current state of a tool for an automated decomposition of Event Processing Language master query into a directed graph of computation will be presented with operator placement strategies, cost model integration, as well as an initial approach for deployment and monitoring of IoT devices used within the healthcare use cases (Pebble Watch, iPhone).&lt;/p&gt;

&lt;p&gt;A range of technology embedded within the project will be covered during the presentation, such as: Esper - CEP engine; Neo4j - graph database; Apache ZooKeeper - reliable distributed coordination service; ActiveMQ - message broker, and others.&lt;/p&gt;

&lt;p&gt;Supervisory Team: Prof. Paul Watson, Dr. Sarah Heaps, Prof. Mike Trenell&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relevant Publications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Michalák, Peter, et al. “Automating computational placement in IoT environments: doctoral symposium.” Proceedings of the 10th ACM International Conference on Distributed and Event-based Systems. ACM, 2016. &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2933435&amp;amp;CFID=928564786&quot;&gt;[Paper.]&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 
</feed>
